<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
	<head lang="en-us">
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />
	<meta name="description" content="Running jobs on a cluster">
	<meta name="generator" content="Hugo 0.24.1" />
	
	<title>Using resources effectively &mdash; Intro to HPC</title>
	
	<link rel="stylesheet" href="https://jstaf.github.io/hpc-intro/css/alabaster.css" type="text/css" />
	<link rel="stylesheet" href="https://jstaf.github.io/hpc-intro/css/highlight.css" type="text/css" />

	

	<link rel="shortcut icon" href="https://jstaf.github.io/hpc-intro/favicon.ico" type="image/x-icon"/>
</head>

	<body role="document">
		<div class="document">
			<div class="documentwrapper">
				<div class="bodywrapper">
					<div class="body" role="main">
						
	<h1>Using resources effectively</h1>
	
	

<p>We now know virtually everything we need to know about getting stuff on a cluster.
We can log on, submit different types of jobs, use preinstalled software,
and install and use software of our own.
What we need to do now is use the systems effectively.</p>

<h2 id="estimating-required-resources-using-the-scheduler">Estimating required resources using the scheduler</h2>

<p>Although we covered requesting resources from the scheduler earlier,
how do we know how much and what type of resources we will need in the first place?</p>

<p>Answer: we don&rsquo;t.
Not until we&rsquo;ve tried it ourselves at least once.
We&rsquo;ll need to benchmark our job and experiment with it before
we know how much it needs in the way of resources.</p>

<p>The most effective way of figuring out how much resources a job needs is to submit a test job,
and then ask the scheduler how many resources it used.
A good rule of thumb is to ask the scheduler for more time and memory than your job can use.
This value is typically two to three times what you think your job will need.</p>

<div class="admonition ">
<p class="first admonition-title">Benchmarking &#39;bowtie2-build&#39;</p>
<p>Create a job that runs the following command
in the same directory as our <em>Drosophila</em> reference genome
from earlier.</p>

<pre><code>bowtie2-build Drosophila_melanogaster.BDGP6.dna.toplevel.fa dmel-index
</code></pre>

<p>The <code>bowtie2-build</code> command is provided by the <code>bowtie2</code> module.
As a reference, this command could use several gigabytes of memory and up to an hour of compute time,
but only 1 cpu in any scenario.</p>

<p>You&rsquo;ll need to figure out a good amount of resources to ask for for this first &ldquo;test run&rdquo;.
You might also want to have the scheduler email you to tell you when the job is done.</p>
</div>

<p>Once the job completes (note that it takes much less time than expected),
we can query the scheduler to see how long our job took and what resources were used.
(Clever users may also notice that the run time is also in the SLURM logfile&hellip;)</p>

<p>We can use <code>sacct</code> to get statistics about our job.</p>

<p>By itself, <code>sacct -u yourUsername</code> shows all commands that we ran
since midnight on the previous day
(we can change this behavior with the <code>--start-time</code> option).</p>

<pre><code>sacct -u yourUsername
</code></pre>

<pre><code>      JobID    JobName  Partition    Account  AllocCPUS      State ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- -------- 
1964               bash   standard    default          1  COMPLETED      0:0 
1964.extern      extern               default          1  COMPLETED      0:0 
1964.0             bash               default          1  COMPLETED      0:0 
1965         build-ind+ summer-sc+    default          1  COMPLETED      0:0 
1965.batch        batch               default          1  COMPLETED      0:0 
1965.extern      extern               default          1  COMPLETED      0:0 
</code></pre>

<p>This shows all the jobs we ran recently
(note that there are multiple entries per job).
To get detailed info about a job,
we change our <code>sacct</code> command slightly
(<code>-j</code> corresponds to job id).</p>

<pre><code>sacct -j 1965 -l
</code></pre>

<p>It will show a ton of info, in fact, every single piece of info
collected on your job by the scheduler.
It may be useful to redirect this information to <code>less</code> to make it easier to view (use the left and right arrow keys to scroll through fields).</p>

<pre><code>sacct -j 1965 -l | less
</code></pre>

<p>Some interesting fields include the following:</p>

<ul>
<li><strong>Hostname</strong> - Where did your job run?</li>
<li><strong>MaxRSS</strong> - What was the maximum amount of memory used?</li>
<li><strong>Elapsed</strong> - How long did the job take?</li>
<li><strong>State</strong> - What is the job currently doing/what happened to it?</li>
<li><strong>MaxDiskRead</strong> - Amount of data read from disk.</li>
<li><strong>MaxDiskWrite</strong> - Amount of data written to disk.</li>
</ul>

<h2 id="measuring-the-statistics-of-currently-running-tasks">Measuring the statistics of currently running tasks</h2>

<p>One very useful feature of SLURM is the ability to SSH
to a node where a job is running and check how it&rsquo;s doing.
To do this, check where a job is running with <code>squeue</code>,
then run <code>ssh nodename</code>.</p>

<p>However, we can also check on stuff running on the login node right now the same way
(so it&rsquo;s not necessary to <code>ssh</code> to a node for this example).</p>

<h3 id="top">top</h3>

<p>The best way to check current system stats is with <code>top</code>
(<code>htop</code> is a prettier version of <code>top</code> but may not be available on your system).</p>

<p>Some sample output from my laptop might look like the following:</p>

<pre><code>Tasks: 296 total,   1 running, 295 sleeping,   0 stopped,   0 zombie
%Cpu(s):  4.3 us,  0.8 sy,  0.0 ni, 93.5 id,  0.8 wa,  0.4 hi,  0.3 si,  0.0 st
KiB Mem :  3948524 total,   119504 free,  1882172 used,  1946848 buff/cache
KiB Swap:  4083708 total,  3909192 free,   174516 used.  1084104 avail Mem 

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                   
 1712 jeff      20   0 3189124 247912  87788 S  13.2  6.3   7:52.10 gnome-shell                                           
12958 jeff      20   0 1109104 141568  54128 S   5.0  3.6   2:15.18 tilix                                             
 2381 jeff      20   0 4209548 178632  29960 S   1.3  4.5   1:29.64 geary                                            
15150 jeff      20   0  153904   4216   3468 R   0.7  0.1   0:00.03 top                                               
   37 root      20   0       0      0      0 S   0.3  0.0   0:01.08 rcuos/3                                              
  612 root     -51   0       0      0      0 S   0.3  0.0   0:07.06 irq/28-INT55D5:                                        
 1326 root      20   0  448704   4928   2424 S   0.3  0.1   0:11.77 docker-containe                                        
13947 jeff      20   0  310624  18368  11556 S   0.3  0.5   0:01.11 hugo                                                      
14288 jeff      20   0  620256 105408  81212 S   0.3  2.7   0:02.33 vivaldi-bin                                             
    1 root      20   0  224048   7868   5452 S   0.0  0.2   0:01.99 systemd                                                   
    2 root      20   0       0      0      0 S   0.0  0.0   0:00.01 kthreadd                                                
    4 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kworker/0:0H                                              
    6 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 mm_percpu_wq                                                   
    7 root      20   0       0      0      0 S   0.0  0.0   0:00.03 ksoftirqd/0         
</code></pre>

<p>Overview of the main fields:</p>

<ul>
<li><code>PID</code> - What is the numerical id of each process?</li>
<li><code>USER</code> - Who started the process?</li>
<li><code>RES</code> - What is the amount of memory currently being used by a process?</li>
<li><code>%CPU</code> - How much of a CPU is each process using? Values higher than 100 percent indicate that a process is running in parallel.</li>
<li><code>%MEM</code> - What percent of system memory is a process using?</li>
<li><code>TIME+</code> - How much CPU time has a process used so far? Processes using 2 CPUs accumulate time at twice the normal rate.</li>
<li><code>COMMAND</code> - What command was used to launch a process?</li>
</ul>

<h3 id="free">free</h3>

<p>Another useful tool is the <code>free -h</code> command.
This will show the currently used/free amount of memory.</p>

<pre><code>free -h
</code></pre>

<pre><code>              total        used        free      shared  buff/cache   available
Mem:           3.8G        1.5G        678M        327M        1.6G        1.6G
Swap:          3.9G        170M        3.7G
</code></pre>

<p>This one is fairly easy to interpret, though keep in mind that <code>buff/cache</code> counts as free memory.</p>

<h3 id="ps">ps</h3>

<p>To show all processes from your current session, type <code>ps</code>.</p>

<pre><code>ps
</code></pre>

<pre><code>  PID TTY          TIME CMD
15113 pts/5    00:00:00 bash
15218 pts/5    00:00:00 ps
</code></pre>

<p>Note that this will only show processes from our current session.
To show all processes of a user, regardless of a session, you can use <code>ps -ef</code> and grep for your username.</p>

<pre><code>ps -ef | grep yourUsername
</code></pre>

<pre><code>jeff      1594     1  0 14:23 ?        00:00:00 /usr/lib/systemd/systemd --user
jeff      1599  1594  0 14:23 ?        00:00:00 (sd-pam)
jeff      1610     1  0 14:23 ?        00:00:01 /usr/bin/gnome-keyring-daemon --daemonize --login
jeff      1627  1586  0 14:23 tty2     00:00:00 /usr/libexec/gdm-wayland-session gnome-session
jeff      1629  1594  0 14:23 ?        00:00:01 /usr/bin/dbus-daemon --session --address=systemd: --nofork --nopidfile --systemd-activation --syslog-only
jeff      1632  1627  0 14:23 tty2     00:00:00 /usr/libexec/gnome-session-binary
jeff      1692  1594  0 14:23 ?        00:00:00 /usr/libexec/gvfsd
</code></pre>

<p>This is useful for identifying which processes are doing what.</p>

<h2 id="killing-processes">Killing processes</h2>

<p>To kill all of a certain type of process, you can run <code>killall commandName</code>.
<code>killall rsession</code> would kill all <code>rsession</code> processes created by RStudio,
for instance.
Note that you can only kill your own processes.</p>

<p>You can also kill processes by their PIDs using <code>kill 1234</code> where <code>1234</code> is a <code>PID</code>.
Sometimes however, killing a process does not work instantly.
To kill the process in the most hardcore manner possible, use the <code>-9</code> flag.
It&rsquo;s recommended to kill using without <code>-9</code> first.
This gives a process the chance to clean up child processes, and exit cleanly.
However, if a process just isn&rsquo;t responding, use <code>-9</code> to kill it instantly.</p>

<h2 id="playing-nice-in-the-sandbox">Playing nice in the sandbox</h2>

<p>You now have everything you need to run jobs, transfer files, use/install software,
and monitor how many resources your jobs are using.</p>

<p>So here are a couple final words to live by:</p>

<ul>
<li><p>Don&rsquo;t run jobs on the login node, though quick tests are generally fine.
A &ldquo;quick test&rdquo; is generally anything that uses less than 10GB of memory, 4 CPUs, and 15 minutes of time.
Remember, the login node is to be shared with other users.</p></li>

<li><p>If someone is being inappropriate and using the login node to run all of their stuff,
message an administrator to kill their stuff.</p></li>

<li><p>Compress files before transferring to save file transfer times with large datasets.</p></li>

<li><p>Use a VCS system like git to keep track of your code. Though most systems have some form
of backup/archival system, you shouldn&rsquo;t rely on it for something as key as your research code.
The best backup system is one you manage yourself.</p></li>

<li><p>Before submitting a run of jobs, submit one as a test first to make sure everything works.</p></li>

<li><p>The less resources you ask for, the faster your jobs will find a slot in which to run.
Lots of small jobs generally beat a couple big jobs.</p></li>

<li><p>You can generally install software yourself, but if you want a shared installation of some kind,
it might be a good idea to message an administrator.</p></li>

<li><p>Always use the default compilers if possible. Newer compilers are great, but older stuff generally
means that your software will still work, even if a newer compiler is loaded.</p></li>
</ul>



						
					</div>
				</div>
			</div>
			
			<div class="sphinxsidebar" role="navigation" aria-label="main navigation">
	<div class="sphinxsidebarwrapper">
		<p class="logo">
			<a href="https://jstaf.github.io/hpc-intro/">
				<img class="logo" src="https://jstaf.github.io/hpc-intro/favicon.ico" alt="Logo"/>
				<h1 class="logo logo-name">Intro to HPC</h1>
			</a>
		</p>
		
		<p class="blurb">Running jobs on a cluster</p>

		

	

	

	
		

		

<h3>Navigation</h3>
<ul>
	
	<li class="toctree-l1">
		<a class="reference internal" href="/hpc-intro/">Intro to High-Performance Computing</a>
	</li>
	
	<li class="toctree-l1">
		<a class="reference internal" href="/hpc-intro/cluster/">What is a cluster</a>
	</li>
	
	<li class="toctree-l1">
		<a class="reference internal" href="/hpc-intro/scheduler/">Scheduling jobs</a>
	</li>
	
	<li class="toctree-l1">
		<a class="reference internal" href="/hpc-intro/modules/">Accessing software</a>
	</li>
	
	<li class="toctree-l1">
		<a class="reference internal" href="/hpc-intro/transferring-files/">Transferring Files</a>
	</li>
	
	<li class="toctree-l1">
		<a class="reference internal" href="/hpc-intro/resources/">Using resources effectively</a>
	</li>
	
</ul>


		<h3>Related Topics</h3>
<ul>
  <li><a href="https://jstaf.github.io/hpc-intro/">Documentation overview</a><ul>
  
  
</ul>

	</div>
</div>
<div class="clearer"></div>
</div>
			<div class="footer">
	&copy; 2017 <a href="https://github.com/jstaf">Jeff Stafford</a>
	|
	Powered by <a href="http://gohugo.io/">Hugo 0.24.1</a>
	&amp; <a href="https://github.com/digitalcraftsman/hugo-alabaster-theme">Alabaster</a>
	
</div>




			

			<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/highlight.min.js"></script>
			<script>hljs.initHighlightingOnLoad();</script>
			

			
		</div>
	</body>
</html>